{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, randn\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows:  1000000\n",
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0| 0.1709497137955568| -0.8664700627108758|\n",
      "|  1| 0.8051143958005459| -0.5970491018333267|\n",
      "|  2| 0.5775925576589018| 0.18267161219540898|\n",
      "|  3| 0.9476047869880925| -1.8497305679917546|\n",
      "|  4|    0.2093704977577|  0.9410417279045351|\n",
      "|  5|0.36664222617947817| -0.6516475674670159|\n",
      "|  6| 0.8078688178371882|  0.5901002135239671|\n",
      "|  7| 0.7135143433452461|  -1.850241871360443|\n",
      "|  8| 0.7195325566306053| 0.09176896733073023|\n",
      "|  9|0.31335292311175456|-0.38605118617831075|\n",
      "| 10| 0.8062503712025726|  1.2134544166783332|\n",
      "| 11|0.10814914646176654| -1.0757702531630617|\n",
      "| 12| 0.3362232980701172| 0.04961226872064977|\n",
      "| 13| 0.8133304803837667|  -0.768259602441542|\n",
      "| 14|0.47649428738170896|  0.2911293146907403|\n",
      "| 15|  0.524728096293865|-0.33406080411047484|\n",
      "| 16| 0.9701253460019921|  1.3607097640771781|\n",
      "| 17| 0.6232167713919952|  0.5986772981082732|\n",
      "| 18| 0.5089687568245219|-0.35158579838711623|\n",
      "| 19| 0.5467504094508642| -0.9115825072509143|\n",
      "+---+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonWordCount\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sqlc = SQLContext(spark.sparkContext)\n",
    "\n",
    "df = (sqlc.range(0, 1000 * 1000)\n",
    "      .withColumn('uniform', rand(seed=10))\n",
    "      .withColumn('normal', randn(seed=27)))\n",
    "\n",
    "print('# rows: ', df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+\n",
      "|summary|                id|             uniform|              normal|\n",
      "+-------+------------------+--------------------+--------------------+\n",
      "|  count|           1000000|             1000000|             1000000|\n",
      "|   mean|          499999.5|  0.4997785318606761|6.545992003465573E-4|\n",
      "| stddev|288675.27893234405|  0.2887560412263698|  1.0003498848232582|\n",
      "|    min|                 0|2.710561290975022E-7|  -4.949492960499273|\n",
      "|    max|            999999|  0.9999998822463074|   4.474351963425938|\n",
      "+-------+------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cor(rand2, rand2):  1.0\n",
      "cor(rand1, rand2):  0.0013720661952388604\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import rand, randn\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonWordCount\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "\n",
    "df = sqlContext.range(0, 1000 * 1000).withColumn('rand1', rand(seed=10)).withColumn('rand2', rand(seed=27))\n",
    "\n",
    "print('cor(rand2, rand2): ', df.stat.corr('rand2', 'rand2'))\n",
    "print('cor(rand1, rand2): ', df.stat.corr('rand1', 'rand2'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
